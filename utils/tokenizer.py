'''Contain SpaCy default tokenize and bnlp basic tokenize
Code of Basic tokenize is taken from, 

# https://github.com/sagorbrur/bnlp/blob/master/bnlp/tokenizer/basic.py
'''
import json
from spacy.lang.bn import Bengali
from spacy.tokenizer import Tokenizer

"""
Basic Tokenization
Basic tokenization tokenize sentence using white spaces, punctuation mark
Code shamelessly copied from BERT tokenization 
To check Original Code: https://github.com/google-research/bert/blob/master/tokenization.py
"""
import six
import unicodedata


def convert_to_unicode(text):
  """Converts `text` to Unicode (if it's not already), assuming utf-8 input."""
  if six.PY3:
    if isinstance(text, str):
      return text
    elif isinstance(text, bytes):
      return text.decode("utf-8", "ignore")
    else:
      raise ValueError("Unsupported string type: %s" % (type(text)))
  elif six.PY2:
    if isinstance(text, str):
      return text.decode("utf-8", "ignore")
    elif isinstance(text, unicode):
      return text
    else:
      raise ValueError("Unsupported string type: %s" % (type(text)))
  else:
    raise ValueError("Not running on Python2 or Python 3?")


def whitespace_tokenize(text):
  """Runs basic whitespace cleaning and splitting on a piece of text."""
  text = text.strip()
  # print("Text: ", text)
  if not text:
    return []
  tokens = text.split()
  # print("tokens : ", tokens)
  return tokens


def _is_punctuation(char):
  """Checks whether `chars` is a punctuation character."""
  cp = ord(char)
  # We treat all non-letter/number ASCII as punctuation.
  # Characters such as "^", "$", and "`" are not in the Unicode
  # Punctuation class but we treat them as punctuation anyways, for
  # consistency.
  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or
      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):
    return True
  cat = unicodedata.category(char)
  if cat.startswith("P"):
    return True
  return False


class BasicTokenizer:
  """Runs basic tokenization (punctuation splitting, lower casing, etc.)."""

  def tokenize(self, text):
    """Tokenizes a piece of text."""
    text = convert_to_unicode(text)


    orig_tokens = whitespace_tokenize(text)
    # print("original tokens: ", orig_tokens)
    split_tokens = []
    for token in orig_tokens:
      # if self.do_lower_case:
      #   token = token.lower()
      #   token = self._run_strip_accents(token)
      split_tokens.extend(self._run_split_on_punc(token))

    # print("split tokens: ", split_tokens)
    output_tokens = whitespace_tokenize(" ".join(split_tokens))
    return output_tokens

  def _run_strip_accents(self, text):
    """Strips accents from a piece of text."""
    text = unicodedata.normalize("NFD", text)
    output = []
    for char in text:
      cat = unicodedata.category(char)
      if cat == "Mn":
        continue
      output.append(char)
    return "".join(output)

  def _run_split_on_punc(self, text):
    """Splits punctuation on a piece of text."""
    chars = list(text)
    i = 0
    start_new_word = True
    output = []
    while i < len(chars):
      char = chars[i]
      if _is_punctuation(char):
        output.append([char])
        start_new_word = True
      else:
        if start_new_word:
          output.append([])
        start_new_word = False
        output[-1].append(char)
      i += 1

    return ["".join(x) for x in output]


def using_spacy(text):
    nlp = Bengali()

    tokenizer = Tokenizer(nlp.vocab)

    tokens = tokenizer(text)

    for token in tokens:
        print(token)

bst = BasicTokenizer()

def bnlp_tokenizer(text):
    tokens = bst.tokenize(text)
    return tokens

def check_jsonl_file():
    data = 'data/main.jsonl'

    with open(data, 'r') as f:
        lines = f.readlines()
    match = 0
    for line in lines[:]:
        line = json.loads(line)
        text, labels = line[0], line[1]
        tokens = bst.tokenize(text)

        if len(tokens) != len(labels):
            print(tokens, 'length', len(tokens))
            print(labels, 'lenght', len(labels))
            print('---')
            match += 1

    print(f"match {match} of total {len(lines)}")


if __name__ == '__main__':
    check_jsonl_file()